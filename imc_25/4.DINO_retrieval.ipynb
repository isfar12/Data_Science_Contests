{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05e899e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4748333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to C:\\Users\\LENOVO/.cache\\torch\\hub\\main.zip\n",
      "C:\\Users\\LENOVO/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\LENOVO/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\LENOVO/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to C:\\Users\\LENOVO/.cache\\torch\\hub\\checkpoints\\dinov2_vitb14_pretrain.pth\n",
      "100%|██████████| 330M/330M [00:31<00:00, 11.0MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load DINOv2 model (ViT-B/14)\n",
    "model = torch.hub.load(\n",
    "    \"facebookresearch/dinov2\",\n",
    "    \"dinov2_vitb14\",\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115cc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b06742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (214, 768)\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"../imc_25/image-matching-challenge-2025/train/imc2024_lizard_pond\"\n",
    "image_names = os.listdir(DATASET_DIR)\n",
    "embeddings = []\n",
    "embedding_names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in image_names:\n",
    "        if not name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            continue   # safety check\n",
    "\n",
    "        path = os.path.join(DATASET_DIR, name)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        feat = model(img_tensor)                      # [1, D]\n",
    "        feat = torch.nn.functional.normalize(feat, dim=1)\n",
    "\n",
    "        embeddings.append(feat.cpu().numpy()[0])\n",
    "        embedding_names.append(name)\n",
    "\n",
    "embeddings = np.stack(embeddings)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206e57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = embeddings @ embeddings.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f1fc36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference image: lizard_00003.png\n",
      "Top-K neighbors:\n",
      "  lizard_00527.png  similarity=0.722\n",
      "  lizard_00698.png  similarity=0.667\n",
      "  lizard_00544.png  similarity=0.628\n",
      "  lizard_00538.png  similarity=0.607\n",
      "  lizard_00051.png  similarity=0.601\n",
      "  lizard_00531.png  similarity=0.596\n",
      "  lizard_00519.png  similarity=0.590\n",
      "  lizard_00233.png  similarity=0.573\n",
      "  lizard_00013.png  similarity=0.572\n",
      "  lizard_00107.png  similarity=0.566\n",
      "  lizard_00516.png  similarity=0.551\n",
      "  lizard_00226.png  similarity=0.541\n",
      "  lizard_00160.png  similarity=0.541\n",
      "  lizard_00512.png  similarity=0.541\n",
      "  lizard_00034.png  similarity=0.537\n"
     ]
    }
   ],
   "source": [
    "K = 15  # number of neighbors to retrieve\n",
    "idx = 0  # reference image index\n",
    "\n",
    "sims = similarity_matrix[idx]\n",
    "topk_idx = np.argsort(-sims)[1:K+1]\n",
    "\n",
    "print(\"Reference image:\", embedding_names[idx])\n",
    "print(\"Top-K neighbors:\")\n",
    "for i in topk_idx:\n",
    "    print(f\"  {embedding_names[i]}  similarity={sims[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9c0e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example image: lizard_00003.png\n",
      "Top neighbors: ['lizard_00527.png', 'lizard_00698.png', 'lizard_00544.png', 'lizard_00538.png', 'lizard_00051.png', 'lizard_00531.png', 'lizard_00519.png', 'lizard_00233.png', 'lizard_00013.png', 'lizard_00107.png']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "K = 15  # start with 30 for lizard_pond\n",
    "neighbors = {}\n",
    "\n",
    "for i, name in enumerate(embedding_names):\n",
    "    sims = similarity_matrix[i]\n",
    "    # exclude self, take top-K\n",
    "    topk = np.argsort(-sims)[1:K+1]\n",
    "    neighbors[name] = [embedding_names[j] for j in topk]\n",
    "\n",
    "# quick check\n",
    "some = embedding_names[0]\n",
    "print(\"Example image:\", some)\n",
    "print(\"Top neighbors:\", neighbors[some][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6063147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate pairs: 2113\n",
      "First 5 pairs: [('lizard_00003.png', 'lizard_00013.png'), ('lizard_00003.png', 'lizard_00034.png'), ('lizard_00003.png', 'lizard_00051.png'), ('lizard_00003.png', 'lizard_00087.png'), ('lizard_00003.png', 'lizard_00107.png')]\n"
     ]
    }
   ],
   "source": [
    "pair_set = set()\n",
    "\n",
    "for a, nbrs in neighbors.items():\n",
    "    for b in nbrs:\n",
    "        if a == b:\n",
    "            continue\n",
    "        pair = tuple(sorted((a, b)))\n",
    "        pair_set.add(pair)\n",
    "\n",
    "pairs = sorted(list(pair_set))\n",
    "print(\"Number of candidate pairs:\", len(pairs))\n",
    "print(\"First 5 pairs:\", pairs[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e781c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
