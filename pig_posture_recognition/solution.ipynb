{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4cddd06",
   "metadata": {},
   "source": [
    "## What this notebook does (simple)\n",
    "\n",
    "1) Read train/test CSVs that list pig images, bounding boxes, and posture labels.\n",
    "\n",
    "2) Crop each pig from its bounding box.\n",
    "\n",
    "3) Train an InceptionV3 image classifier on the cropped pigs.\n",
    "\n",
    "4) Run the trained model on test crops and save `inception_submission.csv`.\n",
    "\n",
    "\n",
    "\n",
    "Key files:\n",
    "\n",
    "- `pig_posture_recognition/train.csv`: rows with `row_id`, `image_id`, `width`, `height`, `bbox`, `class_id`.\n",
    "\n",
    "- `pig_posture_recognition/test.csv`: same but no `class_id`.\n",
    "\n",
    "- `pig_posture_recognition/pig_posture_classes.txt`: label names.\n",
    "\n",
    "- Images live in `pig_posture_recognition/train_images` and `pig_posture_recognition/test_images`.\n",
    "\n",
    "\n",
    "\n",
    "Main hyperparameters (change if needed):\n",
    "\n",
    "- Image size `IMGSZ = 299`\n",
    "\n",
    "- Batch size `BATCH = 16`\n",
    "\n",
    "- Epochs `EPOCHS = 3`\n",
    "\n",
    "- Learning rate `LR = 2e-4`\n",
    "\n",
    "- DataLoader workers `num_workers = 2` (set to 0 on Kaggle if you see worker errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8663827",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "**Core libraries for the pipeline:**\n",
    "\n",
    "**Data handling:** `pandas` for CSV manipulation, `PIL` for image operations, `json` for parsing bounding boxes\n",
    "\n",
    "**Deep learning:** `torch` (PyTorch framework), `torchvision` for pretrained models and image transforms\n",
    "\n",
    "**Utilities:** `pathlib` for cross-platform paths, `sklearn` for grouped data splitting to prevent leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from typing import Iterable, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060815",
   "metadata": {},
   "source": [
    "## Setup Paths, Device, and Seeds\n",
    "\n",
    "**Reproducibility through seeds:** Neural networks use random initialization and data shuffling. Setting seeds (`SEED=42`) ensures identical results across runs—critical for debugging and comparing experiments.\n",
    "\n",
    "**Device selection:** PyTorch can run on CPU or GPU. `torch.cuda.is_available()` detects NVIDIA GPUs, enabling 10-100× speedup for training.\n",
    "\n",
    "**Path resolution:** `Path('.').resolve()` gives absolute paths, making code portable across different execution contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d808e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16087ee02f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_DIR / 'pig_posture_recognition'\n",
    "TRAIN_IMAGES = DATA_DIR / 'train_images'\n",
    "TEST_IMAGES = DATA_DIR / 'test_images'\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d141eca",
   "metadata": {},
   "source": [
    "## Load Metadata: CSVs and Class Names\n",
    "\n",
    "**Purpose:** Load training/test data and class label mappings for the pig posture classification task.\n",
    "\n",
    "**Data structure:**\n",
    "- `train.csv`: ~16k rows with columns: `row_id`, `image_id`, `width`, `height`, `bbox`, `class_id`\n",
    "- `test.csv`: ~7k rows (same format but no `class_id` — our prediction target)\n",
    "- `pig_posture_classes.txt`: 5 posture class names, one per line\n",
    "\n",
    "**Key operations:**\n",
    "\n",
    "1. **Load CSVs:** `pd.read_csv()` reads training and test metadata into DataFrames\n",
    "   - Example train row: `train_00000001_0000`, `00000001.jpg`, 1920×1080, `[967.5,331.5,463.0,447.0]`, class_id=3\n",
    "\n",
    "2. **Parse class names:** List comprehension reads class file and strips whitespace\n",
    "   - Result: `['Lateral_lying_left', 'Lateral_lying_right', 'Sitting', 'Standing', 'Sternal_lying']`\n",
    "\n",
    "3. **Create mappings:**\n",
    "   - `id_to_name`: {0: 'Lateral_lying_left', 1: 'Lateral_lying_right', ...} for converting predictions to readable labels\n",
    "   - `name_to_id`: Reverse mapping for potential string-to-ID conversions\n",
    "\n",
    "**Output:** Display dataset sizes and mappings as sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cc0a550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16062,\n",
       " 6872,\n",
       " ['Lateral_lying_left',\n",
       "  'Lateral_lying_right',\n",
       "  'Sitting',\n",
       "  'Standing',\n",
       "  'Sternal_lying'],\n",
       " {0: 'Lateral_lying_left',\n",
       "  1: 'Lateral_lying_right',\n",
       "  2: 'Sitting',\n",
       "  3: 'Standing',\n",
       "  4: 'Sternal_lying'},\n",
       " {'Lateral_lying_left': 0,\n",
       "  'Lateral_lying_right': 1,\n",
       "  'Sitting': 2,\n",
       "  'Standing': 3,\n",
       "  'Sternal_lying': 4})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata\n",
    "\n",
    "train_df = pd.read_csv(DATA_DIR / 'train.csv')\n",
    "test_df = pd.read_csv(DATA_DIR / 'test.csv')\n",
    "class_names = [c.strip() for c in (DATA_DIR / 'pig_posture_classes.txt').read_text().splitlines() if c.strip()]\n",
    "\n",
    "id_to_name = {i: name for i, name in enumerate(class_names)}\n",
    "name_to_id = {v: k for k, v in id_to_name.items()}\n",
    "\n",
    "len(train_df), len(test_df), class_names, id_to_name, name_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa474e93",
   "metadata": {},
   "source": [
    "## Bounding Box Conversion: From YOLO Format to PIL Crop Coordinates\n",
    "\n",
    "**The coordinate system problem:**\n",
    "\n",
    "Images use pixel coordinates starting from (0, 0) at the **top-left corner**. Our CSV stores bounding boxes in **YOLO format**: `[center_x, center_y, width, height]` — the center point plus dimensions. However, PIL's `crop()` function needs **corner format**: `(x1, y1, x2, y2)` — top-left corner + bottom-right corner coordinates.\n",
    "\n",
    "**Visual representation:**\n",
    "```\n",
    "Image coordinate system:          YOLO format stores:\n",
    "(0,0) ───────────> x              • Center point (cx, cy)\n",
    "  │                                • Box dimensions (w, h)\n",
    "  │    ┌─────────┐\n",
    "  │    │  (cx,cy)│                We need to convert to:\n",
    "  │    │    ●────┤ h              • Top-left corner (x1, y1)  \n",
    "  │    │    │    │                • Bottom-right corner (x2, y2)\n",
    "  v    └────┴────┘\n",
    "  y         w\n",
    "```\n",
    "\n",
    "**Conversion formula (step-by-step):**\n",
    "\n",
    "Given a bounding box `[center_x, center_y, width, height]`, we calculate corner coordinates by moving half the width/height in each direction from the center:\n",
    "\n",
    "1. **Top-left corner (x1, y1):**\n",
    "   - `x1 = center_x - width/2` → Move left by half the width\n",
    "   - `y1 = center_y - height/2` → Move up by half the height\n",
    "\n",
    "2. **Bottom-right corner (x2, y2):**\n",
    "   - `x2 = center_x + width/2` → Move right by half the width\n",
    "   - `y2 = center_y + height/2` → Move down by half the height\n",
    "\n",
    "**Concrete example:**\n",
    "```\n",
    "Input bbox: [967.5, 331.5, 463, 447]\n",
    "           center_x=967.5, center_y=331.5, width=463, height=447\n",
    "\n",
    "Step 1 - Calculate x-coordinates:\n",
    "  x1 = 967.5 - 463/2 = 967.5 - 231.5 = 736.0\n",
    "  x2 = 967.5 + 463/2 = 967.5 + 231.5 = 1199.0\n",
    "  \n",
    "Step 2 - Calculate y-coordinates:\n",
    "  y1 = 331.5 - 447/2 = 331.5 - 223.5 = 108.0\n",
    "  y2 = 331.5 + 447/2 = 331.5 + 223.5 = 555.0\n",
    "\n",
    "Output: (736, 108, 1199, 555)\n",
    "        (top-left corner at pixel 736,108; bottom-right at 1199,555)\n",
    "```\n",
    "\n",
    "**Edge case handling (critical for robustness):**\n",
    "\n",
    "Images have finite dimensions (e.g., 1920×1080). Bounding boxes near edges might go outside image boundaries, causing crop errors. We clamp coordinates:\n",
    "\n",
    "```python\n",
    "x1 = max(0, x_c - w/2)          # Can't go left of pixel 0\n",
    "y1 = max(0, y_c - h/2)          # Can't go above pixel 0\n",
    "x2 = min(img_w, x_c + w/2)      # Can't exceed image width\n",
    "y2 = min(img_h, y_c + h/2)      # Can't exceed image height\n",
    "```\n",
    "\n",
    "**Example edge case:**\n",
    "```\n",
    "Image: 1920×1080 pixels\n",
    "Bbox: [50, 100, 200, 150]  (box near top-left edge)\n",
    "\n",
    "Without clamping:\n",
    "  x1 = 50 - 100 = -50  ❌ Invalid (negative)\n",
    "  \n",
    "With clamping:\n",
    "  x1 = max(0, -50) = 0  ✓ Valid (starts at edge)\n",
    "```\n",
    "\n",
    "**Why integer conversion?**\n",
    "- PIL's `crop()` requires integer pixel coordinates (you can't crop at pixel 736.5)\n",
    "- `int()` truncates decimal → `int(736.0) = 736`\n",
    "- Minor precision loss is acceptable for image processing\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `IMGSZ = 299`: InceptionV3's required input size (architecture-specific constraint)\n",
    "- `BATCH = 16`: Process 16 images simultaneously (GPU memory vs. speed tradeoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cee804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12902, 3160)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMGSZ = 299\n",
    "BATCH = 16\n",
    "\n",
    "\n",
    "def bbox_to_xyxy(bbox: Iterable[float], img_w: int, img_h: int) -> Tuple[int, int, int, int]:\n",
    "\n",
    "    x_c, y_c, w, h = bbox\n",
    "\n",
    "    x1 = max(0, x_c - w / 2)\n",
    "    y1 = max(0, y_c - h / 2)\n",
    "    x2 = min(img_w, x_c + w / 2)\n",
    "    y2 = min(img_h, y_c + h / 2)\n",
    "\n",
    "    return int(x1), int(y1), int(x2), int(y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f366bf",
   "metadata": {},
   "source": [
    "## PyTorch Dataset Class: Custom Data Pipeline\n",
    "\n",
    "**Why custom datasets?** PyTorch's `Dataset` class provides a standardized interface for data loading. We need custom logic because:\n",
    "1. Images need cropping via bounding boxes (not standard preprocessing)\n",
    "2. Each sample requires parsing JSON, computing corners, and on-the-fly cropping\n",
    "3. Train and test datasets return different outputs (labels vs. row_ids)\n",
    "\n",
    "**The three required methods:**\n",
    "\n",
    "**1. `__init__(self, df, transform, img_root, has_label)`**\n",
    "- Stores metadata (DataFrame, transforms, image folder path)\n",
    "- `reset_index(drop=True)` ensures 0-based indexing for reliable access\n",
    "\n",
    "**2. `__len__(self)`**\n",
    "- Returns dataset size (e.g., 16,000 training samples)\n",
    "- DataLoader uses this to calculate: `num_batches = len(dataset) / batch_size`\n",
    "\n",
    "**3. `__getitem__(self, idx)`**\n",
    "- Core logic: given an index, return one preprocessed sample\n",
    "- **Pipeline:** CSV row → parse bbox → open image → crop pig → apply transforms → return (image_tensor, label)\n",
    "\n",
    "**Example flow for idx=100:**\n",
    "```\n",
    "Step 1: row = df.iloc[100]  # Get row 100\n",
    "Step 2: bbox = [967.5, 331.5, 463, 447]  # Parse JSON\n",
    "Step 3: corners = (736, 108, 1199, 555)  # Convert format\n",
    "Step 4: crop = image.crop(corners)  # Extract pig\n",
    "Step 5: tensor = transform(crop)  # Resize to 299×299, normalize\n",
    "Step 6: return (tensor, class_id=3)  # Standing posture\n",
    "```\n",
    "\n",
    "**Memory efficiency:** Crops are computed on-the-fly (not stored), saving RAM and enabling dynamic augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6756df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PigPostureDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, transform, img_root: Path, has_label: bool = True):\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.img_root = img_root\n",
    "        self.has_label = has_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        bbox = json.loads(row['bbox'])\n",
    "\n",
    "        x1, y1, x2, y2 = bbox_to_xyxy(bbox, row['width'], row['height'])\n",
    "\n",
    "        with Image.open(self.img_root / row['image_id']).convert('RGB') as im:\n",
    "\n",
    "            crop = im.crop((x1, y1, x2, y2))\n",
    "\n",
    "        image = self.transform(crop)\n",
    "\n",
    "        if self.has_label:\n",
    "            return image, int(row['class_id'])\n",
    "\n",
    "        return image, row['row_id']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742aa1c",
   "metadata": {},
   "source": [
    "### Cell 13: Image Transforms — Detailed Breakdown\n",
    "\n",
    "**What this cell does:** Defines preprocessing pipelines for training vs validation/test images.\n",
    "\n",
    "**Why transforms?** Neural networks need:\n",
    "1. Consistent input size (299×299)\n",
    "2. Normalized pixel values (ImageNet mean/std)\n",
    "3. Data augmentation (training only) to prevent overfitting\n",
    "\n",
    "**Training Transforms (`train_tfms`):**\n",
    "\n",
    "`train_tfms = T.Compose([...])` — Chains multiple transforms sequentially\n",
    "\n",
    "1. `T.Resize(int(IMGSZ * 1.15))`\n",
    "   - Resize to 343×343 (299 × 1.15 ≈ 343)\n",
    "   - **Why larger than 299?** Next step randomly crops 299×299 from 343×343, creating variation\n",
    "   - Maintains aspect ratio by resizing both dimensions equally\n",
    "\n",
    "2. `T.RandomResizedCrop(IMGSZ, scale=(0.8, 1.0), ratio=(0.8, 1.2))`\n",
    "   - Randomly crops a region that's 80-100% of image area\n",
    "   - Aspect ratio varies between 0.8:1 and 1.2:1\n",
    "   - Resizes that crop to 299×299\n",
    "   - **Effect:** Simulates different zoom levels and perspectives (forces model to recognize pigs at various scales/angles)\n",
    "\n",
    "3. `T.RandomHorizontalFlip()`\n",
    "   - 50% chance to flip image left↔right\n",
    "   - **Why:** Pig postures look similar when flipped; this doubles effective dataset size\n",
    "   - Note: Lateral_lying_left vs right are labeled separately, but flipping helps learn spatial features\n",
    "\n",
    "4. `T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.05)`\n",
    "   - Randomly adjusts color properties:\n",
    "     - **Brightness:** ±15% (simulates different lighting)\n",
    "     - **Contrast:** ±15% (simulates camera/exposure differences)\n",
    "     - **Saturation:** ±10% (color intensity variation)\n",
    "     - **Hue:** ±5% (slight color shifts)\n",
    "   - **Why:** Makes model robust to lighting/camera variations\n",
    "\n",
    "5. `T.ToTensor()`\n",
    "   - Converts PIL Image (H×W×C, 0-255) to PyTorch tensor (C×H×W, 0.0-1.0)\n",
    "   - Changes: NumPy/PIL format → PyTorch format, uint8 → float32, [0,255] → [0,1]\n",
    "\n",
    "6. `T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])`\n",
    "   - Standardizes pixel values using ImageNet statistics\n",
    "   - **Formula:** `pixel_new = (pixel_old - mean) / std`\n",
    "   - **Why these values?** InceptionV3 was pretrained on ImageNet with these stats; using same normalization ensures compatibility\n",
    "   - **Result:** Each channel has ~mean=0, ~std=1\n",
    "\n",
    "**Validation/Test Transforms (`val_tfms`):**\n",
    "\n",
    "`val_tfms = T.Compose([...])`\n",
    "\n",
    "1. `T.Resize(int(IMGSZ * 1.15))`\n",
    "   - Directly resize to 343×343 (no randomness)\n",
    "\n",
    "2. `T.CenterCrop(IMGSZ)`\n",
    "   - Crops center 299×299 region (deterministic, no randomness)\n",
    "   - **Why center?** Most important part of image usually in center; consistent evaluation\n",
    "\n",
    "3. `T.ToTensor()` — Same as train\n",
    "\n",
    "4. `T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])` — Same as train\n",
    "\n",
    "**Key difference:** Training uses augmentation (random crops, color jitter, flips) to artificially expand the dataset; val/test use deterministic transforms for reproducible evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_tfms = T.Compose([\n",
    "\n",
    "    T.Resize(int(IMGSZ * 1.15)),\n",
    "    T.RandomResizedCrop(IMGSZ, scale=(0.8, 1.0), ratio=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1, hue=0.05),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize(int(IMGSZ * 1.15)),\n",
    "    T.CenterCrop(IMGSZ),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81106e02",
   "metadata": {},
   "source": [
    "### Cell 15: Train/Validation Split and DataLoaders — Detailed Breakdown\n",
    "\n",
    "**What this cell does:** Splits train data into training (80%) and validation (20%) sets while keeping all crops from the same image together, then creates efficient data loaders.\n",
    "\n",
    "**Why split?** We need:\n",
    "- **Training set:** To update model weights\n",
    "- **Validation set:** To check if model generalizes (not just memorizing training data)\n",
    "\n",
    "**Critical constraint:** Same image can have multiple crops (multiple pigs). We MUST keep all crops from one image in the same split (train OR val, never both) to prevent data leakage.\n",
    "\n",
    "**Step 1: Grouped Split**\n",
    "\n",
    "1. `splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=SEED)`\n",
    "   - **Class:** scikit-learn's GroupShuffleSplit\n",
    "   - **test_size=0.2:** 20% of data goes to validation (80% to training)\n",
    "   - **n_splits=1:** Only create 1 split (we just need 1 train/val partition)\n",
    "   - **random_state=SEED:** Reproducible split (same split every run with same seed)\n",
    "   - **Key feature:** Respects groups—keeps all samples with same group ID together\n",
    "\n",
    "2. `train_idx, val_idx = next(splitter.split(train_df, groups=train_df['image_id']))`\n",
    "   - **`split(...)`:** Generates train/val indices\n",
    "   - **`groups=train_df['image_id']`:** Group by image_id (e.g., `00000001.jpg`)\n",
    "     - All rows with `image_id='00000001.jpg'` stay together\n",
    "     - If `00000001.jpg` has 3 pig crops, all 3 go to train OR all 3 go to val\n",
    "   - **`next(...)`:** Get the first (and only) split\n",
    "   - **Result:** `train_idx` = array of row indices for training, `val_idx` = array for validation\n",
    "\n",
    "3. `train_split = train_df.iloc[train_idx].copy()`\n",
    "   - Select rows at train_idx positions\n",
    "   - `.copy()` creates independent DataFrame (avoid SettingWithCopyWarning)\n",
    "   - **Result:** ~12.8k rows (80% of 16k)\n",
    "\n",
    "4. `val_split = train_df.iloc[val_idx].copy()`\n",
    "   - Same as above but with validation indices\n",
    "   - **Result:** ~3.2k rows (20% of 16k)\n",
    "\n",
    "**Step 2: Create Datasets**\n",
    "\n",
    "5. `train_ds = PigPostureDataset(train_split, train_tfms, TRAIN_IMAGES, has_label=True)`\n",
    "   - Create dataset with training DataFrame and training transforms (augmentation)\n",
    "   - `has_label=True` means this data has class_id column\n",
    "\n",
    "6. `val_ds = PigPostureDataset(val_split, val_tfms, TRAIN_IMAGES, has_label=True)`\n",
    "   - Create dataset with validation DataFrame and validation transforms (no augmentation)\n",
    "\n",
    "**Step 3: DataLoader Configuration**\n",
    "\n",
    "7. `num_workers = 2`\n",
    "   - Number of parallel subprocesses to load data\n",
    "   - **Why 2?** Small value to avoid multiprocessing issues on Windows while still gaining some speedup\n",
    "   - **Trade-off:** Higher = faster loading but more memory + potential crashes\n",
    "\n",
    "**Step 4: Create DataLoaders**\n",
    "\n",
    "8. `train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=num_workers, pin_memory=True)`\n",
    "   - **train_ds:** Training dataset (~12.8k samples)\n",
    "   - **batch_size=BATCH:** Load 16 images at a time\n",
    "   - **shuffle=True:** Randomize order each epoch\n",
    "     - **Why?** Prevents model from learning order patterns; better generalization\n",
    "     - Each epoch sees data in different order\n",
    "   - **num_workers=2:** Use 2 worker processes for parallel loading\n",
    "   - **pin_memory=True:** Pin memory in RAM for faster GPU transfer\n",
    "\n",
    "9. `val_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=num_workers, pin_memory=True)`\n",
    "   - **val_ds:** Validation dataset (~3.2k samples)\n",
    "   - **shuffle=False:** Keep consistent order for reproducible validation\n",
    "     - Validation should be deterministic—same order every time\n",
    "\n",
    "10. `len(train_ds), len(val_ds)`\n",
    "    - Displays dataset sizes to verify split worked correctly\n",
    "    - **Expected:** (~12800, ~3200) or similar 80/20 ratio\n",
    "    - DataLoader length = ceil(dataset_size / batch_size), e.g., ceil(12800/16) = 800 batches\n",
    "\n",
    "**Why GroupShuffleSplit matters:** Without grouping, we might have:\n",
    "- Train: crop #1 from image A\n",
    "- Val: crop #2 from same image A\n",
    "- **Problem:** Model sees image A during training, then \"evaluates\" on same image A—unfair advantage, validation accuracy would be artificially high!\n",
    "\n",
    "With grouping: All crops from image A go to train OR val, never both—fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d42b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/val split grouped by image_id to avoid leakage across crops\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=SEED)\n",
    "train_idx, val_idx = next(splitter.split(train_df, groups=train_df['image_id']))\n",
    "train_split = train_df.iloc[train_idx].copy()\n",
    "val_split = train_df.iloc[val_idx].copy()\n",
    "\n",
    "\n",
    "train_ds = PigPostureDataset(train_split, train_tfms, TRAIN_IMAGES, has_label=True)\n",
    "val_ds = PigPostureDataset(val_split, val_tfms, TRAIN_IMAGES, has_label=True)\n",
    "\n",
    "\n",
    "\n",
    "num_workers = 2  # keep small for Windows\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4ef70",
   "metadata": {},
   "source": [
    "## Transfer Learning with InceptionV3\n",
    "\n",
    "**Why transfer learning?** Training from scratch requires millions of labeled images and weeks of GPU time. Instead, we leverage ImageNet knowledge (1.4M images, 1000 classes) and adapt it to our 5-class pig problem.\n",
    "\n",
    "**InceptionV3 architecture:**\n",
    "- 48 convolutional layers with \"inception modules\" (parallel filters of different sizes)\n",
    "- Pretrained on ImageNet: already recognizes edges, textures, shapes, and object parts\n",
    "- Input: 299×299 RGB images (architecture requirement)\n",
    "- Original output: 1000 classes\n",
    "\n",
    "**Our modifications:**\n",
    "```python\n",
    "base_model = models.inception_v3(weights=IMAGENET1K_V1, aux_logits=True)\n",
    "base_model.fc = nn.Linear(2048, 5)  # Replace final layer\n",
    "```\n",
    "\n",
    "**What we keep:** All 47 convolutional layers with ImageNet weights → feature extraction backbone\n",
    "\n",
    "**What we replace:** Final fully-connected layer (1000 outputs → 5 outputs)\n",
    "\n",
    "**Training strategy:**\n",
    "1. Freeze conv layers initially? No — we fine-tune ALL weights (better accuracy on small datasets)\n",
    "2. Lower learning rate (2e-4) prevents catastrophic forgetting of ImageNet features\n",
    "\n",
    "**aux_logits=True:** InceptionV3 has an auxiliary classifier at layer 17 (combats vanishing gradients). During training, loss = main_loss + 0.3 × aux_loss. During inference, we ignore auxiliary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3d723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception expects aux_logits=True with weights; grab main logits below\n",
    "\n",
    "base_model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
    "base_model.fc = nn.Linear(base_model.fc.in_features, len(class_names))\n",
    "base_model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711dd0e4",
   "metadata": {},
   "source": [
    "## Training Configuration: Loss, Optimizer, Scheduler\n",
    "\n",
    "**Hyperparameters:**\n",
    "- `EPOCHS = 3`: Full passes through training data (more = better fit, but risk overfitting)\n",
    "- `LR = 2e-4`: Learning rate (step size for weight updates)\n",
    "\n",
    "**Loss function: CrossEntropyLoss**\n",
    "- Combines softmax + negative log-likelihood for multi-class classification\n",
    "- Formula: `loss = -log(softmax(logits)[true_class])`\n",
    "- Example: Model outputs `[0.1, 0.05, 0.2, 0.6, 0.05]`, true class=3 → loss = -log(0.6) ≈ 0.51\n",
    "- Lower loss = more confident correct predictions\n",
    "\n",
    "**Optimizer: AdamW**\n",
    "- Adaptive learning rates per parameter (automatically adjusts based on gradient history)\n",
    "- \"W\" = decoupled weight decay (L2 regularization): penalizes large weights to prevent overfitting\n",
    "- `weight_decay=1e-4`: Adds small penalty proportional to weight magnitude\n",
    "\n",
    "**Learning rate scheduler: CosineAnnealingLR**\n",
    "- Gradually decreases LR following a cosine curve: LR(epoch) = LR_max × 0.5 × (1 + cos(π × epoch/T_max))\n",
    "- Why? Early training benefits from larger steps (far from optimum), late training needs fine adjustments\n",
    "- Example schedule: Epoch 1: 0.0002 → Epoch 2: 0.0001 → Epoch 3: 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b80d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "LR = 2e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(base_model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa8211",
   "metadata": {},
   "source": [
    "## Training/Validation Epoch Runner\n",
    "\n",
    "**Purpose:** Unified function to run one complete pass through data (training with weight updates OR validation without updates).\n",
    "\n",
    "**Key steps:**\n",
    "1. **Set mode:** `model.train(True)` enables dropout/batch-norm training behavior; `model.eval()` disables it\n",
    "2. **Batch loop:** Process 16 images at a time (efficient parallelization)\n",
    "3. **Forward pass:** Input images → model outputs 5 class scores per image\n",
    "4. **Compute loss:** How far are predictions from true labels?\n",
    "5. **Backward pass (training only):**\n",
    "   - `optimizer.zero_grad()`: Clear old gradients\n",
    "   - `loss.backward()`: Compute new gradients via backpropagation\n",
    "   - `optimizer.step()`: Update all 25M weights based on gradients\n",
    "6. **Track metrics:** Accumulate loss and count correct predictions\n",
    "7. **Return:** Average loss and accuracy for the epoch\n",
    "\n",
    "**InceptionV3 quirk:** Returns `(main_output, auxiliary_output)` tuple during training. We extract `outputs[0]` to use only the main classifier.\n",
    "\n",
    "**Example outputs:**\n",
    "- Training: loss=0.65, acc=0.75 (75% correct, weights updated 800 times)\n",
    "- Validation: loss=0.80, acc=0.72 (72% correct, no weight changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf970ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, train_mode: bool):\n",
    "\n",
    "    model.train(mode=train_mode)\n",
    "\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, targets in loader:\n",
    "\n",
    "        images, targets = images.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(train_mode):\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            if isinstance(outputs, tuple):  # main logits, aux logits\n",
    "                outputs = outputs[0]\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            if train_mode:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        correct += (outputs.argmax(1) == targets).sum().item()\n",
    "\n",
    "        total += targets.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29163721",
   "metadata": {},
   "source": [
    "### Cell 23: Training Loop with Checkpointing — Detailed Breakdown\n",
    "\n",
    "**What this cell does:** Trains the model for multiple epochs, validates after each epoch, saves the best model, and prints progress.\n",
    "\n",
    "**Initialization:**\n",
    "\n",
    "1. `best_acc = 0.0`\n",
    "   - **Track best validation accuracy** seen so far\n",
    "   - Initialize to 0.0 (any trained model will beat this)\n",
    "   - **Purpose:** Only save model when it improves (avoids saving worse versions)\n",
    "\n",
    "2. `best_path = PROJECT_DIR / 'best_inception.pt'`\n",
    "   - **Path where we'll save the best model**\n",
    "   - **Location:** Same folder as notebook\n",
    "   - **Extension:** .pt is standard for PyTorch models\n",
    "   - **Contains:** Model weights + validation accuracy\n",
    "\n",
    "**Main Training Loop:**\n",
    "\n",
    "3. `for epoch in range(1, EPOCHS + 1):`\n",
    "   - **Iterate through epochs:** 1, 2, 3 (human-readable numbering)\n",
    "   - **Each epoch:** Complete pass through all training data + validation\n",
    "\n",
    "**Step 4: Train for one epoch**\n",
    "\n",
    "4. `train_loss, train_acc = run_epoch(base_model, train_loader, train_mode=True)`\n",
    "   - **Call run_epoch with train_mode=True**\n",
    "   - **What happens inside:**\n",
    "     1. Loop through all 800 training batches\n",
    "     2. Forward pass → compute loss → backward pass → update weights\n",
    "     3. Track loss and accuracy across all batches\n",
    "   - **Returns:** \n",
    "     - `train_loss`: Average loss on training set (e.g., 0.65)\n",
    "     - `train_acc`: Fraction correctly classified (e.g., 0.75 = 75%)\n",
    "   - **Time:** Takes most of the epoch time (updating weights is slow)\n",
    "\n",
    "**Step 5: Validate**\n",
    "\n",
    "5. `val_loss, val_acc = run_epoch(base_model, val_loader, train_mode=False)`\n",
    "   - **Call run_epoch with train_mode=False (evaluation mode)**\n",
    "   - **What happens inside:**\n",
    "     1. Loop through all 200 validation batches\n",
    "     2. Forward pass → compute loss → NO backward pass, NO weight updates\n",
    "     3. Track loss and accuracy\n",
    "   - **Returns:**\n",
    "     - `val_loss`: Average loss on validation set (e.g., 0.80)\n",
    "     - `val_acc`: Fraction correctly classified (e.g., 0.72 = 72%)\n",
    "   - **Time:** Faster than training (no backprop)\n",
    "   - **Purpose:** Check if model generalizes (not just memorizing training data)\n",
    "\n",
    "**Step 6: Adjust learning rate**\n",
    "\n",
    "6. `scheduler.step()`\n",
    "   - **Update learning rate** using CosineAnnealingLR\n",
    "   - **Effect:** LR gradually decreases each epoch\n",
    "     - Epoch 1: LR ≈ 0.00020\n",
    "     - Epoch 2: LR ≈ 0.00010\n",
    "     - Epoch 3: LR ≈ 0.00002\n",
    "   - **Why?** Fine-tune with smaller steps as training progresses\n",
    "\n",
    "**Step 7: Save best model (checkpoint)**\n",
    "\n",
    "7. `if val_acc > best_acc:`\n",
    "   - **Check:** Did validation accuracy improve?\n",
    "   - **Example:** \n",
    "     - Epoch 1: val_acc=0.70 > best_acc=0.00 → Save\n",
    "     - Epoch 2: val_acc=0.73 > best_acc=0.70 → Save\n",
    "     - Epoch 3: val_acc=0.72 < best_acc=0.73 → Don't save\n",
    "   - **Why check validation?** We care about generalization, not just training performance\n",
    "\n",
    "8. `best_acc = val_acc`\n",
    "   - **Update best accuracy** to current validation accuracy\n",
    "   - **Example:** best_acc = 0.73\n",
    "\n",
    "9. `torch.save({'model_state': base_model.state_dict(), 'acc': val_acc}, best_path)`\n",
    "   - **Save checkpoint to disk**\n",
    "   \n",
    "   - **`base_model.state_dict()`:** Dictionary of all model parameters\n",
    "     - Keys: layer names (e.g., 'Conv2d_1a_3x3.conv.weight')\n",
    "     - Values: weight tensors\n",
    "     - ~25 million parameters for InceptionV3\n",
    "   \n",
    "   - **`{'model_state': ..., 'acc': ...}`:** Save multiple items in one file\n",
    "     - 'model_state': The model weights\n",
    "     - 'acc': Validation accuracy (for reference)\n",
    "   \n",
    "   - **`torch.save(..., best_path)`:** Serialize to file\n",
    "     - **Format:** PyTorch's binary format (pickled tensors)\n",
    "     - **Size:** ~90-100 MB (depends on model)\n",
    "   \n",
    "   - **Result:** File `best_inception.pt` contains the best model so far\n",
    "   - **Why?** If training crashes or we stop early, we can load the best model\n",
    "\n",
    "**Step 8: Print progress**\n",
    "\n",
    "10. `print(f\"Epoch {epoch}/{EPOCHS} | train loss {train_loss:.4f} acc {train_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")`\n",
    "    - **Display training progress** for human monitoring\n",
    "    - **Format:**\n",
    "      - `{epoch}/{EPOCHS}`: e.g., \"2/3\"\n",
    "      - `{train_loss:.4f}`: 4 decimal places, e.g., \"0.6542\"\n",
    "      - `{train_acc:.3f}`: 3 decimal places, e.g., \"0.753\"\n",
    "    - **Example output:**\n",
    "      ```\n",
    "      Epoch 1/3 | train loss 0.6542 acc 0.753 | val loss 0.7831 acc 0.702\n",
    "      Epoch 2/3 | train loss 0.4201 acc 0.841 | val loss 0.5123 acc 0.829\n",
    "      Epoch 3/3 | train loss 0.3012 acc 0.892 | val loss 0.4856 acc 0.845\n",
    "      ```\n",
    "    - **What to look for:**\n",
    "      - Train loss/acc should improve (decrease/increase)\n",
    "      - Val acc should improve (if not, might be overfitting)\n",
    "      - Gap between train/val indicates overfitting (train much better than val)\n",
    "\n",
    "**Final output:**\n",
    "\n",
    "11. `best_acc, best_path`\n",
    "    - **Display best validation accuracy** achieved\n",
    "    - **Display path** where best model is saved\n",
    "    - **Example:** `(0.845, PosixPath('/path/to/best_inception.pt'))`\n",
    "    - **Purpose:** Quick confirmation of training success\n",
    "\n",
    "**Summary of training flow:**\n",
    "1. Epoch 1: Train → Validate → Save (first model always best) → Print → Decrease LR\n",
    "2. Epoch 2: Train → Validate → Save if better → Print → Decrease LR\n",
    "3. Epoch 3: Train → Validate → Save if better → Print\n",
    "4. Load best model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7613b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0.0\n",
    "\n",
    "best_path = PROJECT_DIR / 'best_inception.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "\n",
    "    train_loss, train_acc = run_epoch(base_model, train_loader, train_mode=True)\n",
    "    val_loss, val_acc = run_epoch(base_model, val_loader, train_mode=False)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({'model_state': base_model.state_dict(), 'acc': val_acc}, best_path)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | train loss {train_loss:.4f} acc {train_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")\n",
    "\n",
    "\n",
    "best_acc, best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99fcd9",
   "metadata": {},
   "source": [
    "### Cell 25: Inference and Submission Generation — Detailed Breakdown\n",
    "\n",
    "**What this cell does:** Loads the best trained model, makes predictions on test set, and creates a CSV submission file.\n",
    "\n",
    "**Step 1: Load Best Model**\n",
    "\n",
    "1. `checkpoint = torch.load(best_path, map_location=DEVICE)`\n",
    "   - **Load saved checkpoint** from disk\n",
    "   - **`torch.load(...)`:** Deserializes PyTorch file\n",
    "   - **`map_location=DEVICE`:** Ensures tensors load to correct device\n",
    "     - If DEVICE='cuda': load to GPU\n",
    "     - If DEVICE='cpu': load to CPU (even if saved on GPU)\n",
    "   - **Result:** Dictionary `{'model_state': state_dict, 'acc': 0.845}`\n",
    "\n",
    "2. `base_model.load_state_dict(checkpoint['model_state'])`\n",
    "   - **Restore model weights** from checkpoint\n",
    "   - **`checkpoint['model_state']`:** The state_dict we saved earlier\n",
    "   - **Effect:** All model parameters now match the best epoch\n",
    "   - **Why?** Current model might be from epoch 3, but best was epoch 2\n",
    "\n",
    "3. `base_model.eval()`\n",
    "   - **Set model to evaluation mode**\n",
    "   - **Effect:**\n",
    "     - Disables dropout (no random neuron dropping)\n",
    "     - Batch norm uses running statistics (not batch statistics)\n",
    "   - **Why?** Ensures deterministic, consistent predictions\n",
    "\n",
    "**Step 2: Create Test DataLoader**\n",
    "\n",
    "4. `test_ds = PigPostureDataset(test_df, val_tfms, TEST_IMAGES, has_label=False)`\n",
    "   - **Create test dataset**\n",
    "   - **test_df:** Test CSV (~7k rows with no class_id column)\n",
    "   - **val_tfms:** Use validation transforms (no augmentation, deterministic)\n",
    "   - **TEST_IMAGES:** Folder with test images\n",
    "   - **has_label=False:** Dataset returns (image, row_id) instead of (image, class_id)\n",
    "\n",
    "5. `test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)`\n",
    "   - **Batch test data**\n",
    "   - **shuffle=False:** CRITICAL for submission—maintain row order\n",
    "   - **Why?** Predictions must align with row_ids\n",
    "   - **num_workers=2:** Parallel loading for speed\n",
    "   - **Result:** ~437 batches (7000 / 16 ≈ 437)\n",
    "\n",
    "**Step 3: Inference Loop**\n",
    "\n",
    "6. `preds, row_ids = [], []`\n",
    "   - **Initialize lists** to collect results\n",
    "   - **preds:** Will store predicted class_ids (0-4)\n",
    "   - **row_ids:** Will store corresponding row_ids from test.csv\n",
    "\n",
    "7. `with torch.no_grad():`\n",
    "   - **Disable gradient computation**\n",
    "   - **Effect:** Saves memory, speeds up inference\n",
    "   - **Why?** We're not training, so no need for gradients\n",
    "   - **Context manager:** Automatically re-enables gradients after block\n",
    "\n",
    "8. `for images, ids in test_loader:`\n",
    "   - **Loop through test batches**\n",
    "   - **images:** Batch of image tensors (16, 3, 299, 299)\n",
    "   - **ids:** Batch of row_id strings (16,)\n",
    "   - **Note:** ids are strings like 'test_00000001_0000', not class labels\n",
    "\n",
    "9. `images = images.to(DEVICE)`\n",
    "   - **Move images to GPU/CPU** where model is located\n",
    "   - **Required:** Model and data must be on same device\n",
    "\n",
    "10. `outputs = base_model(images)`\n",
    "    - **Forward pass:** Run images through model\n",
    "    - **Input:** (16, 3, 299, 299) tensor\n",
    "    - **Output:** Usually (16, 5) tensor of logits\n",
    "    - **Special case:** InceptionV3 returns tuple `(main_logits, aux_logits)`\n",
    "\n",
    "11. `if isinstance(outputs, tuple): outputs = outputs[0]`\n",
    "    - **Handle InceptionV3 auxiliary logits**\n",
    "    - **If tuple:** Extract main output, ignore auxiliary\n",
    "    - **Result:** outputs is (16, 5) tensor of logits\n",
    "\n",
    "12. `pred = outputs.argmax(1).cpu().tolist()`\n",
    "    - **Convert logits to class predictions**\n",
    "    \n",
    "    - **`outputs.argmax(1)`:** Find class with highest logit for each sample\n",
    "      - Input: (16, 5) logits\n",
    "      - `argmax(1)`: argmax along dimension 1 (classes)\n",
    "      - Output: (16,) tensor of class indices\n",
    "    \n",
    "    - **`.cpu()`:** Move tensor to CPU (required before converting to Python)\n",
    "      - If already on CPU, this is a no-op\n",
    "    \n",
    "    - **`.tolist()`:** Convert tensor to Python list\n",
    "      - Example: `tensor([3, 1, 4, 0, ...])` → `[3, 1, 4, 0, ...]`\n",
    "    \n",
    "    - **Result:** List of predicted class_ids (integers 0-4)\n",
    "\n",
    "13. `preds.extend(pred)`\n",
    "    - **Append predictions** from this batch to main list\n",
    "    - **extend vs append:** extend adds each element, append would add entire list as one element\n",
    "\n",
    "14. `row_ids.extend(ids)`\n",
    "    - **Append row_ids** from this batch to main list\n",
    "    - **Critical:** Maintains alignment between predictions and row_ids\n",
    "\n",
    "**Step 4: Create Submission File**\n",
    "\n",
    "15. `submission = pd.DataFrame({'row_id': row_ids, 'class_id': preds})`\n",
    "    - **Build DataFrame** with required columns\n",
    "    - **Columns:**\n",
    "      - 'row_id': Test identifiers (e.g., 'test_00000001_0000')\n",
    "      - 'class_id': Predicted posture classes (0-4)\n",
    "    - **Example:**\n",
    "      ```\n",
    "      row_id                  class_id\n",
    "      test_00000001_0000      3\n",
    "      test_00000001_0001      1\n",
    "      test_00000002_0000      4\n",
    "      ```\n",
    "    - **Length:** ~7000 rows (matches test.csv)\n",
    "\n",
    "16. `submission.to_csv('inception_submission.csv', index=False)`\n",
    "    - **Write to CSV file**\n",
    "    - **Filename:** 'inception_submission.csv' in current directory\n",
    "    - **index=False:** Don't write DataFrame index as a column\n",
    "    - **Format:** CSV with header row\n",
    "    - **Result:** File ready for Kaggle submission\n",
    "\n",
    "17. `submission.head()`\n",
    "    - **Display first 5 rows** for quick visual check\n",
    "    - **Purpose:** Verify format looks correct before submitting\n",
    "    - **Example output:**\n",
    "      ```\n",
    "                   row_id  class_id\n",
    "      0  test_00000001_0000         3\n",
    "      1  test_00000001_0001         1\n",
    "      2  test_00000002_0000         4\n",
    "      3  test_00000002_0001         0\n",
    "      4  test_00000003_0000         2\n",
    "      ```\n",
    "\n",
    "**Class ID mapping (reminder):**\n",
    "- 0: Lateral_lying_left\n",
    "- 1: Lateral_lying_right\n",
    "- 2: Sitting\n",
    "- 3: Standing\n",
    "- 4: Sternal_lying\n",
    "\n",
    "**Critical details:**\n",
    "- **Order matters:** shuffle=False ensures predictions align with row_ids\n",
    "- **Best model:** We loaded checkpoint from best validation epoch, not final epoch\n",
    "- **Deterministic:** eval() mode + no augmentation = same predictions every run\n",
    "- **Batch processing:** Much faster than one image at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on test set and submission\n",
    "\n",
    "checkpoint = torch.load(best_path, map_location=DEVICE)\n",
    "base_model.load_state_dict(checkpoint['model_state'])\n",
    "\n",
    "base_model.eval()\n",
    "\n",
    "\n",
    "test_ds = PigPostureDataset(test_df, val_tfms, TEST_IMAGES, has_label=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "preds, row_ids = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, ids in test_loader:\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = base_model(images)\n",
    "\n",
    "        if isinstance(outputs, tuple):\n",
    "\n",
    "            outputs = outputs[0]\n",
    "\n",
    "        pred = outputs.argmax(1).cpu().tolist()\n",
    "        preds.extend(pred)\n",
    "        row_ids.extend(ids)\n",
    "\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({'row_id': row_ids, 'class_id': preds})\n",
    "\n",
    "submission.to_csv('inception_submission.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
